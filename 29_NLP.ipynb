{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NPL\n",
    "---\n",
    "#### Elo notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. Text is inherently unstructured and has all sorts of tricks required for converting (vectorizing) text into a format that a machine learning algorithm can interpret.\n",
    "\n",
    "#### Information Retrieval \n",
    "\n",
    "Information retrieval (IR) Ranking of documents via a search query, is the activity of obtaining information resources relevant to an information need from a collection of information resources. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds.\n",
    "\n",
    "Web search engines are the most visible IR applications.\n",
    "\n",
    "An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "The bag-of-words model is a n-gram model, with n=1. The bag of words is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. \n",
    "\n",
    "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
    "\n",
    "The Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text (Vectorization). \n",
    "\n",
    "To address the problem for common words like \"the\", \"a\", \"to\" are almost always the terms with highest frequeny in the text **\"normalize\"** the term frequencies is to weight a term by the __inverse of document frequency__, or **tf–idf**.\n",
    "\n",
    "#### N-gram model\n",
    "\n",
    "Bag-of-word model is an orderless document representation—only the counts of words mattered. The n-gram model can be used to store spatial information within the text. Applying a __bigram__ model will parse the text into two words units and store the term frequency of each unit as before.\n",
    "\n",
    "#### Sentiment analysis\n",
    "\n",
    "Extract subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.\n",
    "\n",
    "#### Spam filter\n",
    "\n",
    "Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail (\"ham\"). \n",
    "\n",
    "Imagine that there are two literal bags full of words. One bag is filled with words found in spam messages, and the other bag is filled with words found in legitimate e-mail. While any given word is likely to be found somewhere in both bags, the \"spam\" bag will contain spam-related words such as \"stock\", \"Viagra\", and \"buy\" much more frequently, while the \"ham\" bag will contain more words related to the user's friends or workplace.\n",
    "\n",
    "To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be.\n",
    "\n",
    "#### First dimension: mathematical basis\n",
    "\n",
    "Set-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\n",
    "- Standard Boolean model\n",
    "- Extended Boolean model\n",
    "- Fuzzy retrieval\n",
    "\n",
    "Algebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\n",
    "- Vector space model\n",
    "- Generalized vector space model\n",
    "- (Enhanced) Topic-based Vector Space Model\n",
    "- Extended Boolean model\n",
    "- Latent semantic indexing a.k.a. latent semantic analysis\n",
    "\n",
    "Probabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.\n",
    "- Binary Independence Model\n",
    "- Probabilistic relevance model on which is based the okapi (BM25) relevance function\n",
    "- Uncertain inference\n",
    "- Language models\n",
    "- Divergence-from-randomness model\n",
    "- Latent Dirichlet allocation\n",
    "\n",
    "Feature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\n",
    "\n",
    "#### Second dimension: properties of the model\n",
    "\n",
    "- Models without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\n",
    "\n",
    "\n",
    "- Models with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\n",
    "\n",
    "\n",
    "- Models with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision PPV\n",
    "\n",
    "Precision or Positive Predicted Value (PPV) and recall (TPR)\n",
    "\n",
    "Precision is the fraction of the documents retrieved that are relevant to the user's information need.\n",
    "\n",
    "$ {\\displaystyle {\\mbox{precision}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{retrieved documents}}\\}|}}} $\n",
    "\n",
    "In binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. \n",
    "\n",
    "Note that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall TPR\n",
    "\n",
    "Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.\n",
    "\n",
    "${\\displaystyle {\\mbox{recall}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{relevant documents}}\\}|}}}$\n",
    "\n",
    "In binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query.\n",
    "\n",
    "It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fall-out\n",
    "\n",
    "The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\n",
    "\n",
    "$ {\\displaystyle {\\mbox{fall-out}}={\\frac {|\\{{\\mbox{non-relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{non-relevant documents}}\\}|}}} $\n",
    "\n",
    "In binary classification, fall-out is closely related to specificity and is equal to $ {\\displaystyle (1-{\\mbox{specificity}})}$. It can be looked at as the probability that a non-relevant document is retrieved by the query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-score / F-measure\n",
    "\n",
    "The weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is:\n",
    "\n",
    "${\\displaystyle F={\\frac {2\\cdot \\mathrm {precision} \\cdot \\mathrm {recall} }{(\\mathrm {precision} +\\mathrm {recall} )}}}$\n",
    "\n",
    "This is also known as the ${\\displaystyle F_{1}}$ measure, because recall and precision are evenly weighted.\n",
    "\n",
    "The general formula for non-negative real ${\\displaystyle \\beta }$ is:\n",
    "\n",
    "${\\displaystyle F_{\\beta }={\\frac {(1+\\beta ^{2})\\cdot (\\mathrm {precision} \\cdot \\mathrm {recall} )}{(\\beta ^{2}\\cdot \\mathrm {precision} +\\mathrm {recall} )}}\\,}$\n",
    "\n",
    "Two other commonly used $F$ measures are the ${\\displaystyle F_{2}}$ measure, which weights recall twice as much as precision, and the ${\\displaystyle F_{0.5}}$ measure, which weights precision twice as much as recall.\n",
    "\n",
    "The F-measure was derived by van Rijsbergen (1979) so that ${\\displaystyle F_{\\beta }}$ \"measures the effectiveness of retrieval with respect to a user who attaches ${\\displaystyle \\beta }$ times as much importance to recall as precision\". It is based on van Rijsbergen's effectiveness measure ${\\displaystyle E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}}.$ Their relationship is:\n",
    "\n",
    "${\\displaystyle F_{\\beta }=1-E}$ where ${\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}}$\n",
    "\n",
    "F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "In computer science, lexical analysis is the process of converting a sequence of characters (such as in a computer program, web page or document) into a sequence of tokens (strings with an assigned and thus identified meaning) which results in another tokenized document.\n",
    "\n",
    "#### Stop Words\n",
    "\n",
    "In computing, stop words are words which are filtered out before or after processing of natural language data (text). Any group of words can be chosen as the stop words for a given purpose. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\". Other search engines remove some of the most common words—including lexical words, such as \"want\"—from a query in order to improve performance.\n",
    "\n",
    "In information theory, systems are modeled by a transmitter, channel, and receiver. The transmitter produces messages that are sent through the channel. The channel modifies the message in some way. The receiver attempts to infer which message was sent. In this context, entropy (more specifically, Shannon entropy) is the expected value (mean) of the information contained in each message. 'Messages' can be modeled by any flow of information.\n",
    "\n",
    "In information theory/decision trees, features that do not have that much information in them are not worth keeping around. In NLP, these features are called stop words.\n",
    "\n",
    "#### Sentence Segmentation\n",
    "\n",
    "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing.\n",
    "\n",
    "\n",
    "#### Pre processing\n",
    "\n",
    "Text homogenization.\n",
    "\n",
    "#### NGrams\n",
    "\n",
    "An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1) – order Markov model. n-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. Two benefits of n-gram models (and algorithms that use them) are simplicity and scalability – with larger n, a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eloisaelias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = ['alt.atheism', 'sci.electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = fetch_20newsgroups(subset='train', categories=subset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'I love statistics, programming and data science!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'statistics', ',', 'programming', 'and', 'data', 'science', '!']\n"
     ]
    }
   ],
   "source": [
    "print document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "stopw  = stopwords.words('english')\n",
    "print stopw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "cleaning = [word for word in word_list if not word in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmatation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = 'Cross Industry Standard Process for Data Mining. commonly known by its acronym CRISP-DM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segments = segment.tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cross Industry Standard Process for Data Mining.',\n",
       " 'commonly known by its acronym CRISP-DM']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Documents into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def documents(segments):\n",
    "    doc = []\n",
    "    for seg in segments:\n",
    "        doc.append(word_tokenize(seg))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cross', 'Industry', 'Standard', 'Process', 'for', 'Data', 'Mining', '.'],\n",
       " ['commonly', 'known', 'by', 'its', 'acronym', 'CRISP-DM']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result: two documents\n",
    "docs(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'Cross Industry Standard Process for Data Mining'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cross', 'Industry', 'Standard')\n",
      "('Industry', 'Standard', 'Process')\n",
      "('Standard', 'Process', 'for')\n",
      "('Process', 'for', 'Data')\n",
      "('for', 'Data', 'Mining')\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "threegrams = ngrams(sentence.split(), n)\n",
    "for grams in threegrams:\n",
    "    print grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words - Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.301511344578\n",
      "  (0, 4)\t0.301511344578\n",
      "  (0, 9)\t0.301511344578\n",
      "  (0, 7)\t0.301511344578\n",
      "  (0, 2)\t0.301511344578\n",
      "  (0, 6)\t0.301511344578\n",
      "  (0, 1)\t0.301511344578\n",
      "  (0, 5)\t0.301511344578\n",
      "  (0, 10)\t0.301511344578\n",
      "  (0, 8)\t0.301511344578\n",
      "  (0, 3)\t0.301511344578\n"
     ]
    }
   ],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TF-IDF Features\n",
    "\n",
    "TFIDF is a relevance measure. It is used for identifying documents that are related to a search query. A search query itself is also a document.\n",
    "\n",
    "In information retrieval, tf–idf, short for __term frequency–inverse document frequency__, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval, text mining, and user modeling. \n",
    "\n",
    "The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes. For instance, 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n",
    "\n",
    "Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n",
    "\n",
    "#### Term frequency\n",
    "\n",
    "The first form of term weighting is due to Hans Peter Luhn (1957) and is based on the Luhn Assumption:\n",
    "\n",
    "- The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
    "\n",
    "#### Inverse document frequency\n",
    "\n",
    "An inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "\n",
    "- The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "__tf–idf__is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist.\n",
    "\n",
    "\n",
    "#### Ranking\n",
    "\n",
    "Ranking of query results is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. \n",
    "\n",
    "Given a query __q__ and a collection __D__ of documents that match the query, the problem is to rank, that is, sort, the documents in __D__ according to some criterion so that the \"best\" results appear early in the result list displayed to the user. Classically, ranking criteria are phrased in terms of relevance of documents with respect to an information need expressed in the query.\n",
    "\n",
    "Ranking is often reduced to the computation of numeric scores on query/document pairs; a baseline score function for this purpose is the __cosine similarity between tf–idf vectors representing the query and the document in a vector space model__, BM25 scores, or probabilities in a probabilistic IR model. A ranking can then be computed by sorting documents by descending score. \n",
    "\n",
    "An alternative approach is to define a score function on pairs of documents __d₁, d₂__ that is positive if and only if __d₁__ is more relevant to the query than __d₂__ and using this information to sort.\n",
    "\n",
    "Ranking functions are evaluated by a variety of __means__; one of the simplest is determining the precision of the first __k top-ranked__ results for some fixed __k__; for example, the proportion of the top 10 results that are relevant, on average over many queries.\n",
    "\n",
    "Frequently, computation of ranking functions can be simplified by taking advantage of the observation that only the relative order of scores matters, not their absolute value; hence terms or factors that are independent of the document may be removed, and terms or factors that are independent of the query may be precomputed and stored with the document.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
