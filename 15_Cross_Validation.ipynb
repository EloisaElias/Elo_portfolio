{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Cross Validation\n",
    "---\n",
    "#### Elo notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Dependent and independent variables Statistics synonyms](https://en.wikipedia.org/wiki/Dependent_and_independent_variables)\n",
    "\n",
    "Depending on the context, an **independent variable** is sometimes called: \n",
    "\n",
    "- \"predictor variable\"\n",
    "- regressor, \n",
    "- covariate, \n",
    "- \"controlled variable\", \n",
    "- \"manipulated variable\", \n",
    "- \"explanatory variable\", \n",
    "- exposure variable (see reliability theory), \n",
    "- \"risk factor\" (see medical statistics), \n",
    "- \"feature\" (in machine learning and pattern recognition) \n",
    "- \"input variable.\" \n",
    "- In econometrics, the term \"control variable\" is usually used instead of \"covariate\".\n",
    "\n",
    "Depending on the context, a **dependent variable** is sometimes called:\n",
    "\n",
    "- \"response variable\", \n",
    "- \"regressand\", \n",
    "- \"criterion\", \n",
    "- \"predicted variable\", \n",
    "- \"measured variable\", \n",
    "- \"explained variable\", \n",
    "- \"experimental variable\", \n",
    "- \"responding variable\", \n",
    "- \"outcome variable\", \n",
    "- \"output variable\" or \"label\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Residual sum of squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\n",
    "\n",
    "The objective consists of adjusting the parameters of a model function to best fit a data set. The least squares method finds its optimum when the sum, S, of squared residuals is a minimum.\n",
    "\n",
    "${S=\\sum _{i=1}^{n}{r_{i}}^{2}}$\n",
    "\n",
    "A residual is defined as the difference between the actual value of the dependent variable and the value predicted by the modael. Each data point has one residual. \n",
    "\n",
    "$r_{i}=y_{i}-f(x_{i},{\\boldsymbol \\beta })$\n",
    "\n",
    "An example of a model is that of the straight line in two dimensions. Denoting the $y$-intercept as $\\beta _{0}$ and the slope as $\\beta _{1}$, the model function is given by $f(x,\\boldsymbol \\beta)=\\beta_0+\\beta_1 x.$ \n",
    "\n",
    "\n",
    "**The residual is the error that is not explained by the regression equation**\n",
    "\n",
    "It is a measure of the discrepancy between the data and an estimation model\n",
    "\n",
    "A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection.\n",
    "\n",
    "In a model with a single explanatory variable, RSS is given by:\n",
    "\n",
    "$ {\\displaystyle RSS=\\sum _{i=1}^{n}(y_{i}-f(x_{i}))^{2}} $\n",
    "\n",
    "\n",
    "${\\displaystyle RSS=\\sum _{i=1}^{n}(\\varepsilon _{i})^{2}=\\sum _{i=1}^{n}(y_{i}-(\\alpha +\\beta x_{i}))^{2}} $\n",
    "\n",
    "\n",
    "$r(Y,\\hat{Y})^2$ is the proportion of variance in ''Y'' explained by a linear function of ''X''.\n",
    "\n",
    "That equation can be written as:\n",
    "\n",
    "$$r(Y,\\hat{Y})^2 = \\frac{SS_\\text{reg}}{SS_\\text{tot}}\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$SS_\\text{reg}$ is the regression sum of squares, also called the __explained sum of squares__\n",
    "\n",
    "$SS_\\text{tot}$ is the __total sum of squares__ (proportional to the __variance__ of the data)\n",
    "\n",
    "$$SS_\\text{reg}=\\sum_i (\\hat{Y}_i-\\bar{Y})^2$$\n",
    "$$SS_\\text{tot}=\\sum_i (Y_i-\\bar{Y})^2$$\n",
    "\n",
    "The sum of squares of residuals, also called the __residual sum of squares__:\n",
    "\n",
    "\n",
    "\n",
    "$$SS_\\text{res}=\\sum_i (y_i - f_i)^2=\\sum_i (y_i - \\hat{y_i})^2=\\sum_i e_i^2\\,$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$f_i = \\hat{y_i} = f(x_i, \\beta_j)=\\beta_0 + \\sum_{j=1}^p \\beta_j X_{i,j} + \\varepsilon_i$$\n",
    "\n",
    "The most general definition of the coefficient of determination is\n",
    "\n",
    "$$R^2 \\equiv 1 - {SS_{\\rm res}\\over SS_{\\rm tot}}.\\,$$\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "R2 is a statistic that will give some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is a statistical measure of how well the regression line approximates the real data points. An R2 of 1 indicates that the regression line perfectly fits the data.\n",
    "\n",
    "\n",
    "---\n",
    "#### Summary\n",
    "$\\boldsymbol{r_{i}=y_{i}-f(x_{i},{\\boldsymbol \\beta })}$\n",
    "\n",
    "$f(x,\\boldsymbol \\beta)=\\beta_0+\\beta_1 x.$ \n",
    "\n",
    "$\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$   The mean of the observed data\n",
    "\n",
    "$SS_\\text{tot}=\\sum_i (y_i-\\bar{y})^2$  The total sum of squares \n",
    "\n",
    "$SS_\\text{reg}=\\sum_i (\\hat{Y_i} -\\bar{y})^2$   The regression sum of squares, also called the explained sum of squares\n",
    "\n",
    "$ SS_\\text{res}=\\sum_i (y_i - \\hat{Y_i})^2=\\sum_i e_i^2\\ $  The sum of squares of residuals, also called the residual sum of squares\n",
    "\n",
    "$R^2 \\equiv 1 - {SS_{\\rm res}\\over SS_{\\rm tot}}$ **Coefficient of determination** \n",
    "\n",
    "\n",
    "${Y_i = \\beta_0 + \\sum_{j=1}^p {\\beta_j X_{i,j}} + \\varepsilon_i}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IMPORTANT! Errors and residuals](https://en.wikipedia.org/wiki/Errors_and_residuals)\n",
    "\n",
    "**In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"theoretical value\".**\n",
    "\n",
    "One can **standardize statistical errors** (especially of a normal distribution) in a **z-score** (or \"standard score\"), and **standardize residuals in a t-statistic**, or more generally **studentized residuals.** \n",
    "\n",
    "**Important:** \n",
    "\n",
    "The **error ( $\\boldsymbol \\varepsilon$ or disturbance)** of **an observed value** is the deviation of the observed value from the **(unobservable) TRUE VALUE** of a quantity of interest (for example, a **Population Mean**), \n",
    "\n",
    "and the **residual** of **an observed value** is the difference between the observed value and the **ESTIMATED VALUE** of the quantity of interest (for example, a **Sample Mean**). \n",
    "\n",
    "The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals. \n",
    "\n",
    "The expected value, being the mean of the entire population, is typically **unobservable, and hence the statistical error cannot be observed either.**\n",
    "\n",
    "A residual (or fitting deviation), on the other hand, is an **observable estimate of the un observable statistical error.** The sample mean could serve as a good **estimator of the population mean.**\n",
    "\n",
    "Suppose there is a series of observations from a univariate distribution and we want to estimate the mean of that distribution (the so-called location model). In this case, the errors are the deviations of the observations from the population mean, while the residuals are the deviations of the observations from the sample mean.\n",
    "\n",
    "For example, if the mean height in a **population** of 21-year-old men is 1.75 meters, and one randomly chosen man is 1.80 meters tall, then the \"error\" is 0.05 meters; if the randomly chosen man is 1.70 meters tall, then the \"error\" is −0.05 meters. The expected value, being the mean of the entire population, is typically **unobservable, and hence the statistical error cannot be observed either.**\n",
    "\n",
    "- The difference between the height of each man in the sample and the **unobservable population mean is a statistical error**, whereas\n",
    "\n",
    "- The difference between the height of each man in the sample and the **observable sample mean is a residual.**\n",
    "\n",
    "Note that, because of the definition of the sample mean, the sum of the residuals within a random sample is necessarily zero, and thus the residuals are necessarily not independent. The statistical errors, on the other hand, are independent, and their sum within the random sample is almost surely not zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other uses of the word \"error\" in statistics\n",
    "\n",
    "See also: **Bias (statistics)**\n",
    "\n",
    "The use of the term \"error\" as discussed in the sections above is in the sense of a deviation of a value from a hypothetical unobserved value. \n",
    "\n",
    "At least two other uses also occur in statistics, both referring to observable prediction errors:\n",
    "\n",
    "**Mean square error** or mean squared error (**MSE**) and **root mean square error (RMSE)** refer to the amount by which the values predicted by an estimator differ from the quantities being estimated (typically outside the sample from which the model was estimated).\n",
    "\n",
    "** sum of squared residuals (SSR) or residual sum of squares (RSS) or Sum of squared errors(SSE) or  sum of squared errors of prediction (SSE)**, refers to the residual sum of squares (the sum of squared residuals) of a regression; this is the sum of the squares of the deviations of the actual values from the predicted values, within the sample used for estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Mean Square Error MSE](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "\n",
    "The MSE assesses the quality of an **[Estimator](https://en.wikipedia.org/wiki/Estimator) (i.e., a mathematical function mapping a sample of data to a parameter of the population from which the data is sampled)** or a **[Predictor](https://en.wikipedia.org/wiki/Residual_sum_of_squares) (i.e., a function mapping arbitrary inputs to a sample of values of some random variable).** \n",
    "\n",
    "##### SUPER IMPORTANT! Definition of an MSE **differs** according to whether one is describing an **[estimator](https://en.wikipedia.org/wiki/Estimator) or a [predictor](https://en.wikipedia.org/wiki/Residual_sum_of_squares)**\n",
    "\n",
    "MSE is a risk function, corresponding to the expected value of the squared error loss or quadratic loss.\n",
    "\n",
    "**Predictor variable:** Is the independent variable and is sometimes called a , **\"Feature\" (in machine learning and pattern recognition)** or \"input variable **regressor**, **covariate**, \"controlled variable\", \"manipulated variable\", \"explanatory variable\", exposure variable (see reliability theory), \"risk factor\" (see medical statistics).\" In econometrics, the term \"control variable\" is usually used instead of \"covariate\"\n",
    "\n",
    "**Response variable:** Is the dependent variable is sometimes called a **\"Label\" (in machine learning and pattern recognition)**, \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\".\n",
    "\n",
    "\n",
    "#### [Predictor - Related to Residuals](https://en.wikipedia.org/wiki/Residual_sum_of_squares) \n",
    "\n",
    "\n",
    "$ SS_\\text{res}=\\sum_i (y_i - \\hat{Y_i})^2=\\sum_i e_i^2\\ $  The sum of squares of residuals, also called the residual sum of squares\n",
    "\n",
    "If $\\hat{Y}$ is a vector of $n$ predictions, and $Y$ is the vector of observed values of the variable being predicted, then the within-sample MSE of the predictor is computed as\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.}$$\n",
    "\n",
    "I.e., the MSE is the mean ${\\displaystyle \\left({\\frac {1}{n}}\\sum _{i=1}^{n}\\right)}$ of the squares of the errors ${\\displaystyle (Y_{i}-{\\hat {Y_{i}}})^{2}}$ This is an easily computable quantity for a particular sample (and hence is sample-dependent).\n",
    "\n",
    "\n",
    "#### [Estimator](https://en.wikipedia.org/wiki/Estimator)\n",
    "\n",
    "The MSE of an estimator ${\\displaystyle {\\hat {\\theta }}} $ with respect to an unknown parameter ${\\displaystyle \\theta }$ is defined as\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {E} _{\\hat {\\theta }}\\left[({\\hat {\\theta }}-\\theta )^{2}\\right].}$\n",
    "\n",
    "This definition depends on the unknown parameter, and the MSE in this sense is a property of an estimator. Since an MSE is an expectation, it is not a random variable. That being said, the MSE could be a function of unknown parameters, in which case any estimator of the MSE based on estimates of these parameters would be a function of the data and thus a random variable. If the estimator is derived from a sample statistic and is used to estimate some population statistic, then the expectation is with respect to the sampling distribution of the sample statistic.\n",
    "\n",
    "The MSE can be written as the sum of the variance of the estimator and the squared bias of the estimator, providing a useful way to calculate the MSE and implying that in the case of unbiased estimators, the MSE and variance are equivalent.\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {Var} _{\\hat {\\theta }}({\\hat {\\theta }})+\\operatorname {Bias} ({\\hat {\\theta }},\\theta )^{2}.} $\n",
    "\n",
    "\n",
    "For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator)\n",
    "\n",
    "[Bias of an Estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator)\n",
    "\n",
    "**Other uses of the word \"error\" in statistics - See also: [Bias (statistics)](https://en.wikipedia.org/wiki/Bias_(statistics)**\n",
    "\n",
    "$\\operatorname {Bias}^{2}({\\hat f(x_{i}) )}$ means $( \\operatorname {Bias}({\\hat f(x_{i}) )} )^{2}$, **The amount by which the average of our estimate differs from the true mean. Typically the more complex we make the model** $\\hat f$, **the lower the (squared) Bias but the higuer the variance.**\n",
    "\n",
    "**True mean: (unobservable) TRUE VALUE** of a quantity of interest (for example, a **Population Mean**), \n",
    "\n",
    "Statistical bias is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated.\n",
    "\n",
    "A statistic is biased if it is calculated in such a way that it is systematically different from the population parameter being estimated. \n",
    "\n",
    "The bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. Otherwise the estimator is said to be biased. \n",
    "\n",
    "In statistical hypothesis testing, a test is said to be unbiased if, for some alpha level (between 0 and 1), the probability the null is rejected is less than or equal to the alpha level for the entire parameter space defined by the null hypothesis, whilst the probability the null is rejected is greater than or equal to the alpha level for the entire parameter space defined by the alternate hypothesis\n",
    "\n",
    "In statistical hypothesis testing, a test is said to be unbiased if, for some alpha level (between 0 and 1), the probability the null is rejected is less than or equal to the alpha level for the entire parameter space defined by the null hypothesis, whilst the probability the null is rejected is greater than or equal to the alpha level for the entire parameter space defined by the alternate hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, Variance and Mean Squared Error MSE\n",
    "\n",
    "Main article: [Bias–variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "See also: [Accuracy (trueness and precision)](https://en.wikipedia.org/wiki/Accuracy_and_precision#ISO_Definition_(ISO_5725))\n",
    "\n",
    " While bias quantifies the average difference to be expected between an estimator and an underlying parameter, an estimator based on a finite sample can additionally be expected to differ from the parameter due to the randomness in the sample.\n",
    "\n",
    "One measure which is used to try to reflect both types of difference is the mean square error,\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {E} {\\big [}({\\hat {\\theta }}-\\theta )^{2}{\\big ]}.}$\n",
    "\n",
    "This can be shown to be equal to the square of the bias, plus the variance:\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\operatorname {MSE} ({\\hat {\\theta }})=&(\\operatorname {E} [{\\hat {\\theta }}]-\\theta )^{2}+\\operatorname {E} [\\,({\\hat {\\theta }}-\\operatorname {E} [\\,{\\hat {\\theta }}\\,])^{2}\\,]\\\\=&(\\operatorname {Bias} ({\\hat {\\theta }},\\theta ))^{2}+\\operatorname {Var} ({\\hat {\\theta }})\\end{aligned}}}$\n",
    "\n",
    "When the parameter is a vector, an analogous decomposition applies:\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {trace} (\\operatorname {Var} ({\\hat {\\theta }}))+\\left\\Vert \\operatorname {Bias} ({\\hat {\\theta }},\\theta )\\right\\Vert ^{2}} $\n",
    "\n",
    "where\n",
    "\n",
    "${\\displaystyle \\operatorname {trace} (\\operatorname {Var} ({\\hat {\\theta }}))} $\n",
    "\n",
    "is the trace of the covariance matrix of the estimator.\n",
    "\n",
    "An estimator that minimises the bias will not necessarily minimise the mean square error. Sampling distributions of two alternative estimators for a parameter $β_0$. Although $\\hat {β_1}$ is unbiased, it is clearly inferior to the biased $\\hat {β_2}$.\n",
    "\n",
    "**Ridge regression** is one example of a technique where allowing a little bias may lead to a considerable reduction in variance, and more reliable estimates overall.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [Precision and Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#ISO_Definition_(ISO_5725)\n",
    "\n",
    "**[THE VERY BEST!](https://en.wikipedia.org/wiki/Bessel%27s_correction)**: The field of statistics, where the interpretation of measurements plays a central role, prefers to use **the terms Bias and Variance (Variability)** instead of **accuracy and precision**: **Bias is the amount of inaccuracy** and **Variability is the amount of imprecision.**\n",
    "\n",
    "#### Accuracy (Bias) has two definitions:\n",
    "\n",
    "- More commonly, it is a description of **systematic errors**, a **measure of statistical bias (IMPORTANT!)**; as these cause a difference between a result and a \"true\" value, ISO calls this trueness. For the manufacturing industry you reduce Bias with MSA and Test equipment calibration.\n",
    "\n",
    "- Alternatively, ISO defines accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.\n",
    "\n",
    "- Accuracy in Logistic Regression is affected by inbalanced classes.\n",
    "\n",
    "#### Precision (Variance) is a description of random errors, a measure of statistical variability. \n",
    "\n",
    "- The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results.\n",
    "\n",
    "- Precision is the probability of a positive prediction being correct\n",
    "\n",
    "\n",
    "A measurement system is considered valid if it is both accurate and precise. Related terms include bias (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and error (random variability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bias - Variance trade off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "The bias–variance dilemma or problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set\n",
    "\n",
    "- The bias is an error from erroneous assumptions in the learning algorithm. \n",
    "\n",
    "High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "\n",
    "- The variance is an error from sensitivity to small fluctuations in the training set. \n",
    "\n",
    "High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n",
    "\n",
    "This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.\n",
    "\n",
    "The bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately capture the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously.\n",
    "\n",
    " High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with low variance typically produce simpler models that don't tend to overfit but may underfit their training data, failing to capture important regularities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Loss function ( Cost function)](https://en.wikipedia.org/wiki/Loss_function)\n",
    "\n",
    "In statistics, typically a **loss function** is used for **parameter estimation**, and the event in question is some **function of the difference between estimated and true values** for an instance of data. \n",
    "\n",
    "For example: Standard linear regression cost function, in which we obtain the beta coefficients which minimize this function.\n",
    "\n",
    "$ SS_\\text{res}=\\sum_i (y_i - \\hat{Y_i})^2=\\sum_i e_i^2\\ $ \n",
    "\n",
    "In mathematical optimization, statistics, econometrics, decision theory, machine learning and computational neuroscience, \n",
    "\n",
    "a **loss function or cost function** is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
    "\n",
    "An optimization problem seeks to minimize a loss function. \n",
    "\n",
    "An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Reduction Techniques\n",
    "\n",
    "- Subset selection\n",
    "- Shrinkage methods\n",
    "- Dimensionality reduction\n",
    "\n",
    "### [Norm](http://mathworld.wolfram.com/VectorNorm.html)\n",
    "\n",
    "A [norm](https://en.wikipedia.org/wiki/Norm_(mathematics) is a function that assigns a strictly positive length or size to each vector in a vector space—save for the zero vector, which is assigned a length of zero.\n",
    "\n",
    "A simple example is two dimensional Euclidean space R2 equipped with the \"Euclidean norm\" Elements in this vector space are usually drawn as arrows in a 2-dimensional cartesian coordinate system starting at the origin (0, 0). The Euclidean norm assigns to each vector the length of its arrow. Because of this, the Euclidean norm is often known as the magnitude.\n",
    "\n",
    "The Euclidean norm is by far the most commonly used norm on $R^n$, but there are other norms on this vector space as will be shown below. \n",
    "\n",
    "The concept of unit circle (the set of all vectors of norm 1) is different in different norms: for the 1-norm the unit circle in $R^2$ is a [square](https://en.wikipedia.org/wiki/Square), for the 2-norm (Euclidean norm) it is the well-known [unit circle](https://en.wikipedia.org/wiki/Unit_circle), while for the infinity norm it is a different square.\n",
    "\n",
    "### Shrinkage methods\n",
    "\n",
    "### [L1 norm -   Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "\n",
    "Lasso (least absolute shrinkage and selection operator) (also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. \n",
    "\n",
    "[PQSQ](https://github.com/Mirkes/PQSQ-regularized-regression/wiki)\n",
    "\n",
    "The ${\\displaystyle L_{1}}$ norm can be used to approximate the optimal ${\\displaystyle L_{0}}$ norm via convex relaxation. It can be shown that the ${\\displaystyle L_{1}} $ norm induces sparsity. In the case of least squares, this problem is known as LASSO in statistics\n",
    "\n",
    "$$\\min_{w \\in \\mathbb{R}^p} \\frac{1}{n} \\|\\hat X w - \\hat Y \\|^2 + \\lambda \\|w\\|_{1}$$\n",
    "\n",
    "[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "### [L2 norm - Ridge](http://mathworld.wolfram.com/L2-Norm.html)\n",
    "\n",
    "[Relationship with PCA](https://tamino.wordpress.com/2011/02/12/ridge-regression/)\n",
    "\n",
    "[Ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "\n",
    "\n",
    "### [Lp Spaces](https://en.wikipedia.org/wiki/Lp_space)\n",
    "the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces.\n",
    "\n",
    "In penalized regression, 'L1 penalty' and 'L2 penalty' refer to penalizing either the L1 norm of a solution's vector of parameter values (i.e. the sum of its absolute values), or its L2 norm (its Euclidean length). \n",
    "\n",
    "Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. \n",
    "\n",
    "Techniques which use an L2 penalty, like ridge regression, encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.\n",
    "\n",
    "The length of a vector $x = (x1, x2, ..., xn)$ in the n-dimensional real vector space Rn is usually given by the Euclidean norm:\n",
    "\n",
    "$${\\displaystyle \\left\\|x\\right\\|_{2}=\\left({x_{1}}^{2}+{x_{2}}^{2}+\\dotsb +{x_{n}}^{2}\\right)^{1/2}}$$\n",
    "\n",
    "The Euclidean distance between two points x and y is the length ||x − y||2 of the straight line between the two points. In many situations, the Euclidean distance is insufficient for capturing the actual distances in a given space.\n",
    "\n",
    "\n",
    "An analogy to this is suggested by taxi drivers in a grid street plan who should measure distance not in terms of the length of the straight line to their destination, but in terms of the rectilinear distance (L1 norm), which takes into account that streets are either orthogonal or parallel to each other. The class of p-norms generalizes these two examples and has an abundance of applications in many parts of mathematics, physics, and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
    "\n",
    "It is mainly used in settings where the goal is prediction, and one wants to estimate how **accurately** a predictive model will perform in practice. \n",
    "\n",
    "In a prediction problem, a model is usually given a dataset of known data on which training is run **(training dataset)**, and a dataset of unknown data (or first seen data) against which the model is tested **(testing dataset)**. \n",
    "\n",
    "**Non-exhaustive cross-validation**\n",
    "\n",
    "Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-p-out cross-validation.\n",
    "\n",
    "    k-fold cross-validation\n",
    "    2-fold cross-validation\n",
    "    Repeated random sub-sampling validation\n",
    "\n",
    "In **K-fold cross validation**, the data is split into k groups. One group out of the k groups will be the test set, the rest (k-1) groups will be the training set. In the next iteration, another group will be the test set, and the rest will be the training set. The process repeats for k iterations (k-fold). In each fold, a metric for accuracy (MSE in this case) will be calculated and an overall average of that metric will be calculated over k-folds.\n",
    "\n",
    "### Model Improvement\n",
    "\n",
    "- tune parameter 1\n",
    "- tune parameter 2\n",
    "- feature engineering - feature selec+on\n",
    "- scale factors 1 way - scale factors another way\n",
    "- etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model selection](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html)\n",
    "\n",
    "from skelearn \n",
    "\n",
    "Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the Lasso estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learning Curves](https://en.wikipedia.org/wiki/Learning_curve)\n",
    "\n",
    "As the training set gets larger, the error for a quadratic function increases.\n",
    "The error value will plateau out after a certain m, or training set size.\n",
    "\n",
    "If a learning algorithm is suffering from **high bias**, getting more training data **will not (by itself) help much.**\n",
    "\n",
    "\n",
    "#### High Bias\n",
    "\n",
    "**Low training set size:** causes $Jtrain(\\theta)$ to be low and $JCV(\\theta)$ to be high.\n",
    "\n",
    "**Large training set size:** causes both $Jtrain(\\theta)$ and $JCV(\\theta)$ to be high with $Jtrain(\\theta)≈JCV(\\theta).$\n",
    "\n",
    "#### High Variance\n",
    "\n",
    "**Low training set size:** $Jtrain(\\theta)$ will be low and $JCV(\\theta)$ will be high.\n",
    "\n",
    "**Large training set size:** Jtrain(\\theta) increases with training set size and $JCV(\\theta)$ continues to decrease without leveling off. Also, $Jtrain(\\theta) < JCV(\\theta)$ but the difference between them remains significant.\n",
    "\n",
    "If a learning algorithm is suffering from **high variance**, getting more training data is likely to help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Useful links\n",
    "#### Numpy Resources\n",
    "\n",
    "* [Numpy Tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
    "* [Numpy Routines](https://docs.scipy.org/doc/numpy/reference/routines.html#routines)\n",
    "* [NumPy: creating and manipulating numerical data](https://scipy-lectures.github.io/intro/numpy/index.html)\n",
    "* [Numpy Linear Algebra](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html)\n",
    "* [Scipy-Numpy LECTURE](http://www.scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays)\n",
    "* [N-dimensional array](http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html)\n",
    "* [Numpy Scientific Python Lectures](http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb)\n",
    "* [Numpy Broadcasting](http://wiki.scipy.org/EricsBroadcastingDoc)\n",
    "* [Very Basic Numpy array dimension visualization](https://stackoverflow.com/questions/48200911/very-basic-numpy-array-dimension-visualization)\n",
    "* [Crash Course in Python for Scientist](http://nbviewer.ipython.org/gist/rpmuller/5920182)\n",
    "\n",
    "#### In NumPy dimensions are called axes. The number of axes is rank.\n",
    "\n",
    "In an numpy array, a **row vector** is defined as:\n",
    "\n",
    "```python\n",
    "a = np.array([[1, 2, 3]])\n",
    "```\n",
    "The shape of `a` is `(1, 3)`.\n",
    "\n",
    "A **column vector** is defined as:\n",
    "```python\n",
    "b = np.array([[1], [2], [3]])\n",
    "```\n",
    "The shape of `b` is `(3, 1)`.\n",
    "\n",
    "Check the `shape` of all the vectors. If the shape is missing a value, i.e. `(3,)` or  `(,3)`.\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "row_vector = np.arange(0, 10, 1).reshape(1, 10)\n",
    "\n",
    "print row_vector\n",
    ">>> [[0 1 2 3 4 5 6 7 8 9]]\n",
    "\n",
    "print row_vector.shape\n",
    ">>> (1, 10)\n",
    "\n",
    "col_vector = np.linspace(100, 300, 3).reshape(3,1)\n",
    "\n",
    "print col_vector\n",
    ">>> [[ 100.]\n",
    "    [ 200.]\n",
    "    [ 300.]]\n",
    "\n",
    "print col_vector.shape\n",
    ">>> (3, 1)\n",
    "\n",
    "```\n",
    "\n",
    "[[:, 0] from a multidimentional vector to a List of column axis= 1, Index=0 ](https://stackoverflow.com/questions/38556674/why-do-numpy-array-arr2d-1-and-arr2d-0-produce-different-results)\n",
    "\n",
    "#### Rows and Columns\n",
    "\n",
    "`y : m == rows == (axis=0)`\n",
    "\n",
    "`x : n == columns == (axis=1)`\n",
    "\n",
    "`(row, col) == (m, n) == (axis=0, axis=1) == (-y, x)`\n",
    "\n",
    "#### Tensor\n",
    "\n",
    "Create a random 3D tensor, 3 rows and 4 columns (you should create a Matrix except there's an extra dimension)\n",
    "\n",
    "#### Vector in Matrix Form\n",
    "A **column vector** is a matrix with $n$ rows and 1 column and to differentiate from a standard matrix $X$ of higher dimensions can be denoted as a bold lower case $\\boldsymbol{x}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} =\n",
    "  \\begin{bmatrix}\n",
    "    x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and a **row vector** is generally written as the transpose\n",
    "\n",
    "$$\\boldsymbol{x}^T = [x_1, x_2, \\ldots, x_n]$$\n",
    "\n",
    "\n",
    "#### [Numpy Matrix reshaped into a vector](https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa)\n",
    "\n",
    "```\n",
    "Row_vector.shape\n",
    ">> (1, n)\n",
    "\n",
    "Column_vector.shape\n",
    ">> (n, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Scikit-Learn\n",
    "\n",
    "* [sklearn - User guide](http://scikit-learn.org/stable/user_guide.html#user-guide)\n",
    "\n",
    "* [Sklearn - API Reference](http://scikit-learn.org/stable/modules/classes.html)\n",
    "\n",
    "* [Model selection and evaluation](http://scikit-learn.org/stable/model_selection.html) \n",
    "\n",
    "    * [Model evaluation: quantifying the quality of predictions](http://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)\n",
    "\n",
    "* [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "* [Machine Learning in Python](http://scikit-learn.org/stable/)\n",
    "\n",
    "* [Dataset API](http://scikit-learn.org/stable/datasets/index.html)\n",
    "\n",
    "* [Metrics - Pairwise](http://scikit-learn.org/stable/modules/metrics.html)\n",
    "\n",
    "* [sklearn - Preprocesing](http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "**Sklearn general workflow**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load data from csv file\n",
    "df = pd.read_csv('data/housing_prices.csv')\n",
    "X = df[['square_feet', 'num_rooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Run Linear Regression\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print \"Intercept:\", regr.intercept_\n",
    "print \"Coefficients:\", regr.coef_\n",
    "print \"R^2 error:\", regr.score(X_test, y_test)\n",
    "predicted_y = regr.predict(X_test)\n",
    "```\n",
    "\n",
    "**[Polynomial Features](http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html)**\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load data is identical as above\n",
    "df = pd.read_csv('data/housing_prices.csv')\n",
    "X = df[['square_feet', 'num_rooms']]\n",
    "y = df['price']\n",
    "\n",
    "# Add the polynomial features\n",
    "poly = PolynomialFeatures(3)\n",
    "X_new = poly.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.15)\n",
    "\n",
    "# Run Linear Regression (the same as above)\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "print \"Intercept:\", regr.intercept_\n",
    "print \"Coefficients:\", regr.coef_\n",
    "print \"R^2 error:\", regr.score(X_test, y_test)\n",
    "predicted_y = regr.predict(X_test)\n",
    "```\n",
    "\n",
    "#### Pandas\n",
    "\n",
    "[Pandas ecosystem](http://pandas.pydata.org/pandas-docs/version/0.15.0/ecosystem.html#ecosystem-visualization)\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/)\n",
    "\n",
    "using the column names\n",
    "df.loc[:, [‘colname0’, ‘colname1’]]\n",
    "df[[]] # double bracket\n",
    "\n",
    "using the column indices\n",
    "df.iloc[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = load_boston()\n",
    "# data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_features = load_boston().data\n",
    "type(ds_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
       "          0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
       "          6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
       "          2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
       "          4.98000000e+00],\n",
       "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00,\n",
       "          0.00000000e+00,   4.69000000e-01,   6.42100000e+00,\n",
       "          7.89000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
       "          2.42000000e+02,   1.78000000e+01,   3.96900000e+02,\n",
       "          9.14000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_target = load_boston().target\n",
    "ds_target[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [One-fold cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ds_features, ds_target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.088366284197065"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data \n",
    "\n",
    "Assesing how the results of the model will generalize to an independent data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.741098538906176"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test = mean_squared_error(y_test, model.predict(X_test))\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function\n",
    "\n",
    "${\\displaystyle \\operatorname {MSE} ({\\hat {\\theta }})=\\operatorname {E} {\\big [}({\\hat {\\theta }}-\\theta )^{2}{\\big ]}.}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_e(y_true, y_prediction):\n",
    "    serror = (y_true - y_prediction)**2\n",
    "    return sum(serror)/len(serror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.088366284197043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_e(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [K-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "[Sklearn - Computing cross-validated metrics](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model, ds_features, ds_target, cv=10, scoring='neg_mean_squared_error')\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average of MSE from eack K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.763091505422629"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aaa9850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfcv = []\n",
    "for kf in xrange(2, 10):\n",
    "    kfcv.append(cross_val_score(model, ds_features, ds_target, cv=kf, scoring='neg_mean_squared_error').mean()*-1)\n",
    "    \n",
    "plt.plot(kfcv)\n",
    "plt.ylabel('Average MSE per K-fold')\n",
    "plt.xlabel('K-fold');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
