{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) \n",
    "---\n",
    "**Elo notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). \n",
    "\n",
    "Tree models where the target variable can take a **discrete set of values** are called **classification trees**; in these tree structures, **leaves represent class labels** and **branches represent conjunctions of features that lead to those class labels.**\n",
    "\n",
    "Decision Trees (DTs) are a [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm), non-parametric, non linear supervised learning method used for classification (Nominal/Discrete data) and regression (Continuous data). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "\n",
    "**[Decision Tree implementations differ primarily along these axes:](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)**\n",
    "\n",
    "* The splitting criterion (i.e., how \"variance\" is calculated)\n",
    "\n",
    "* Whether it builds models for regression (continuous variables, e.g., a score) as well as classification (discrete variables, e.g., a class label)\n",
    "\n",
    "* Technique to eliminate/reduce over-fitting\n",
    "\n",
    "* Whether it can handle incomplete data\n",
    "\n",
    "\n",
    " **CART**, or Classification And Regression Trees is often used as a generic acronym for the term Decision Tree, though it apparently has a more specific meaning. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\n",
    "\n",
    "### [NP-completeness](https://en.wikipedia.org/wiki/NP-completeness)\n",
    "\n",
    "Data comes in records of the form:\n",
    "\n",
    "$  {\\displaystyle ({\\textbf {x}},Y)=(x_{1},x_{2},x_{3},...,x_{k},Y)} $\n",
    "\n",
    "The dependent variable, $ {\\displaystyle Y}$, is the target variable that we are trying to understand, classify or generalize. The vector $ {\\displaystyle x}$ is composed of the features, $x_{1},x_{2},x_{3}$ ..etc., that are used for that task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Video](https://www.youtube.com/watch?v=AmCV4g7_-QM&list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO&index=3)\n",
    "### In order to pick which feature to split on, we need a way of measuring how good the split is. We select the split by using **Information Gain** or the **Gini Impurity**.\n",
    "\n",
    "[Why are implementations of decision tree algorithms usually binary and what are the advantages of the different impurity metrics?](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md)\n",
    "\n",
    "To arrive to these measurements we need to understand the following: \n",
    "\n",
    "1. Information Gain \n",
    "2. Gini Impurity\n",
    "\n",
    "#### [Gini Impurity vs Entropy](https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy)\n",
    "\n",
    "According to scikit-learn documentation, gini plays the same role as entropy in information gain, rather than information gain itself, which makes the problem much simpler: now it's the question of differences between\n",
    "\n",
    "Gini is intended for continuous attributes and Entropy is for attributes that occur in classes\n",
    "\n",
    "- Gini is to minimize misclassification as it is symetric to 0.5\n",
    "- Entropy is for exploratory analysis, entropy will penalize more small probabilities. (Entropy is a little slower to compute because of the logarithmic function) \n",
    "\n",
    "Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Below are the formulae of both:\n",
    "\n",
    "$ \\displaystyle \\mathrm Gini:Gini(E)= {\\displaystyle =1-\\sum _{i=1}^{J}{p_{i}}^{2}}$\n",
    "\n",
    "$\\displaystyle \\mathrm Entropy:H(E)= -\\sum _{i}p_{i}\\log _{2}p_{i}$\n",
    "\n",
    "notes: \n",
    "\n",
    "- Gini impurity doesn't require to compute logarithmic functions, which are computationally intensive.\n",
    "\n",
    "-  \"Gini method works only when the target variable is a binary variable.\" - Learning Predictive Analytics with Python. \n",
    "\n",
    "\n",
    "\n",
    "### [Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "${\\displaystyle \\operatorname {I} _{G}(p)=1-\\sum _{i=1}^{J}{p_{i}}^{2}}$\n",
    "\n",
    "### [Entropy (Information Entropy or Shannon Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "\n",
    "**Entropy is zero when one outcome is certain.**\n",
    "\n",
    "Intuitively, if a set has all the **same labels, that'll have low entropy** and if it has a **mix of labels, that's high entropy**. \n",
    "\n",
    "We would like to create splits that minimize the entropy in each size. If our splits do a good job splitting along the boundary between classes, they have more predictive power. \n",
    "\n",
    "**The information entropy** is expressed in terms of a discrete set of probabilities $p_i$ so that the measure of information entropy associated with each possible data value is the negative logarithm of the probability mass function for the value: \n",
    "\n",
    "${\\displaystyle \\mathrm {H} (T)=\\operatorname {I} _{E}\\left(p_{1},p_{2},...,p_{J}\\right)=-\\sum _{i=1}^{J}{p_{i}\\log _{2}p_{i}}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\mathrm T$ denote as a set of training examples, each of the form ${\\displaystyle ({\\textbf {x}},y)=(x_{1},x_{2},x_{3},...,x_{k},y)}$ \n",
    "\n",
    "Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more \"information\" than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy. \n",
    "\n",
    "Entropy is a measure of unpredictability of the state, or equivalently, of its average information content. To get an intuitive understanding of these terms, consider the example of a political poll. Usually, such polls happen because the outcome of the poll is not already known. In other words, the outcome of the poll is relatively unpredictable, and actually performing the poll and learning the results gives some new information; these are just different ways of saying that the a priori entropy of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the a priori \n",
    "entropy of the second poll result is small relative to that of the first. \n",
    "\n",
    "\n",
    "\n",
    "### [Information Gain ](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)\n",
    "\n",
    "Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. \n",
    "\n",
    "In general terms, the expected information gain is the change in information entropy $ Î—$ from a prior state to a state that takes some information as given:\n",
    "\n",
    "${\\displaystyle IG(T,a)=\\mathrm {H} {(T)}-\\mathrm {H} {(T|a)}} $\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mathrm {T}$ denote as a set of training examples, each of the form ${\\displaystyle ({\\textbf {x}},y)=(x_{1},x_{2},x_{3},...,x_{k},y)}$. $\\mathrm{T}$ is considered the **parent dataset**.\n",
    "\n",
    "$ {\\displaystyle \\mathrm {H} {(T|a)}}$ is the conditional entropy of ${\\displaystyle T}$ given the value of attribute ${\\displaystyle a}$.\n",
    "\n",
    "Where:\n",
    "\n",
    "${\\displaystyle a}$ is the feature to perform the split. \n",
    "\n",
    "${\\displaystyle \\overbrace {IG(T,a)} ^{\\text{Information Gain}}=\\overbrace {\\mathrm {H} (T)} ^{\\text{Entropy (parent)}}-\\overbrace {\\mathrm {H} (T|a)} ^{\\text{Weighted Sum of Entropy (Children)}}}$\n",
    "\n",
    "$ {\\displaystyle {IG(T,a)} =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}} $\n",
    "\n",
    "where $\\mathrm {H}$ is entropy, $ {\\displaystyle \\mathrm {H} {(T|a)}}$ is the weighted average entropy of several sub domains after observing Event a. (Since in Decision Tree, a tree node will separate a whole domain into several ones and grow the tree iteratively on each node)\n",
    "\n",
    "Now lets define a $\\mathrm {set}$ of training input of $\\mathrm {T}$ with attribute ${\\displaystyle \\mathrm{a}}$ this is considered the **child dataset**. Then the information gain of $\\mathrm {T}$ for attribute ${\\displaystyle \\mathrm{a}}$ is the difference between the a priori Shannon entropy $ {\\displaystyle \\mathrm {H} {(T)}}$ of the training set and the conditional entropy $ {\\displaystyle \\mathrm {H} {(T|a)}}$\n",
    "\n",
    "$ {\\displaystyle  IG(T,a)=\\mathrm {H} (T)- \\sum _{v\\in vals(a)}{{\\frac {|S_{a}{(v)}|}{|T|}}\\cdot \\mathrm {H} \\left(S_{a}{\\left(v\\right)}\\right)}.}$\n",
    "\n",
    "For a value ${\\displaystyle v}$ taken by attribute ${\\displaystyle a}$ , let\n",
    "${\\displaystyle S_{a}{(v)}=\\{{\\textbf {x}}\\in T|x_{a}=v\\}}$\n",
    "\n",
    "The information gain of ${\\displaystyle T}$ given ${\\displaystyle a}$ can be defined as the difference between the unconditional Shannon entropy of ${\\displaystyle T}$ and the expected entropy of ${\\displaystyle T}$ conditioned on ${\\displaystyle a}$\n",
    "\n",
    "\n",
    "### [Information Theory](https://en.wikipedia.org/wiki/Information_theory)\n",
    "\n",
    "\n",
    "**Entropy is zero when one outcome is certain.**\n",
    "\n",
    "The basic idea of information theory is the more one knows about a topic, the less new information one is apt to get about it. If an event is very probable, it is no surprise when it happens and thus provides little new information. Inversely, if the event was improbable, it is much more informative that the event happened. Therefore, the information content is an increasing function of the inverse of the probability of the event $(1/p)$.\n",
    "\n",
    "A key measure in information theory is **\"entropy\"**. Entropy quantifies the amount of uncertainty (information) involved in the value of a random variable or the outcome of a random process**. \n",
    "\n",
    "a measure of information in a single random variable, and mutual information, a measure of information in common between two random variables. \n",
    "\n",
    "Intuitively, the entropy $H(X)$ of a discrete random variable X **is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.**\n",
    "\n",
    "The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "- Boosted trees Incrementally building an ensemble by training each new instance to emphasize the training instances previously mis-modeled. A typical example is AdaBoost. These can be used for regression-type and classification-type problems.\n",
    "\n",
    "- Bootstrap aggregated (or bagged) decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction.\n",
    "    - A random forest classifier is a specific type of bootstrap aggregating\n",
    "\n",
    "- Rotation forest â€“ in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### links\n",
    "\n",
    "\n",
    "#### [Boilerplate](https://en.wikipedia.org/wiki/Boilerplate_code)\n",
    "\n",
    "#### [Python test](http://pythontesting.net/framework/nose/nose-introduction/)\n",
    "\n",
    "#### [Overriding __str__ method](https://www.quora.com/What-is-the-use-of-__str__-in-python)\n",
    "\n",
    "#### [Decision Trees - Sklearn](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "#### [The probability-weighted average ](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean)\n",
    "\n",
    "#### [Expected value](https://en.wikipedia.org/wiki/Expected_value)\n",
    "\n",
    "#### [Categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 'bat'], [2, 'cat'], [2, 'rat'], [3, 'bat']])\n",
    "y = np.array([1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', 'bat'],\n",
       "       ['2', 'cat'],\n",
       "       ['2', 'rat'],\n",
       "       ['3', 'bat']], dtype='|S21')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', 'bat'], dtype='|S21')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a lambda function that checks for True/False in an array\n",
    "tffunc = lambda x: isinstance(x, str) or \\\n",
    "                    isinstance(x, bool) or \\\n",
    "                    isinstance(x, unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tffunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes exactly 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bb210da70dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtffunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() takes exactly 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "tffunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tffunc(X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfarray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-92e5b611d814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfarray' is not defined"
     ]
    }
   ],
   "source": [
    "tfarray(X[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.function_base.vectorize at 0x1a17cfdf90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vectorize(tffunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vectorize(tffunc)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_variables = np.vectorize(tffunc)(X[0])\n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_variables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose.tools\n",
    "from nose.tools import assert_almost_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rain</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook  Temperature  Humidity  Windy      Result\n",
       "0     sunny           85        85  False  Don't Play\n",
       "1     sunny           80        90   True  Don't Play\n",
       "2  overcast           83        78  False        Play\n",
       "3      rain           70        96  False        Play"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Outlook', u'Temperature', u'Humidity', u'Windy', u'Result'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
