{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "**Decision Tree implementations differ primarily along these axes:**\n",
    "\n",
    "* the splitting criterion (i.e., how \"variance\" is calculated)\n",
    "\n",
    "* whether it bulds models for regression (continuous variables, e.g., a score) as well as classification (discrete variables, e.g., a class label)\n",
    "\n",
    "* technique to eliminate/reduce over-fitting\n",
    "\n",
    "* whether it can handle incomplete data\n",
    "\n",
    "\n",
    "**The major Decision Tree implementations are:**\n",
    "\n",
    "* **ID3**, or Iternative Dichotomizer, was the first of three Decision Tree implementations developed by Ross Quinlan (Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106.)\n",
    "\n",
    "\n",
    "* **CART**, or Classification And Regression Trees is often used as a generic acronym for the term Decision Tree, though it apparently has a more specific meaning. In sum, the CART implementation is very similar to C4.5; the one notable difference is that CART constructs the tree based on a numerical splitting criterion recursively applied to the data, whereas C4.5 includes the intermediate step of constructing *rule set*s.\n",
    "\n",
    "\n",
    "* **C4.5**, Quinlan's next iteration. The new features (versus ID3) are: (i) accepts both continuous and discrete features; (ii) handles incomplete data points; (iii) solves over-fitting problem by (very clever) bottom-up technique usually known as \"pruning\"; and (iv) different weights can be applied the features that comprise the training data. Of these, the first three are very important--and i would suggest that any DT implementation you choose have all three. The fourth (differential weighting) is much less important\n",
    "\n",
    "\n",
    "* **C5.0**, the most recent Quinlan iteration. This implementation is covered by patent and probably as a result, is rarely implemented (outside of commercial software packages). I have never coded a C5.0 implementation myself (I have never even seen the source code) so i can't offer an informed comparison of C5.0 versus C4.5. I have always been skeptical about the improvements claimed by its inventor (Ross Quinlan)--for instance, he claims it is \"several orders of magnitude\" faster than C4.5. Other claims are similarly broad (\"significantly more memory efficient\") and so forth. I'll just point you to studies which report the result of comparison of the two techniques and you can decide for yourself.\n",
    "\n",
    "* **CHAID** (chi-square automatic interaction detector) actually predates the origianl ID3 implementation by about six years (published in a Ph.D thesis by Gordon Kass in 1980). I know every little about this technique.The R Platform has a Package called CHAID which includes excellent documentation\n",
    "\n",
    "\n",
    "* **MARS** (multi-adaptive regression splines) is actually a term trademarked by the original inventor of MARS, Salford Systems. As a result, MARS clones in libraries not sold by Salford are named something other than MARS--e.g., in R, the relevant function is polymars in the polyspline library. Matlab and Statistica also have implemetnations with MARS-functiobality\n",
    "\n",
    "\n",
    "* **C4.5** is the Decision Tree flavor implemented in Orange; CART is the flavor in sklearn--both excellent implementations in excellent ML libraries.\n",
    "\n",
    "\n",
    "* **C4.5** is a major step beyond ID3--both in terms of range (C4.5 has a far broader use case spectrum because it can handle continuous variables in the training data) and in terms of model quality.\n",
    "\n",
    "Perhaps the most significant claimed improvement of C5.0 versus C4.5 is support for boosted trees. Ensemble support for DTs--boosted trees and Random Forests--has been included in the DT implementation in Orange; here, ensemble support was added to a C4.5 algorithm.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "In information theory, systems are modeled by a transmitter, channel, and receiver. The transmitter produces messages that are sent through the channel. The channel modifies the message in some way. The receiver attempts to infer which message was sent. In this context, **entropy** (more specifically, Shannon entropy) **is the expected value (average) of the information contained in each message.** \n",
    "\n",
    "**Entropy of a random variable, a fundamental notion in information theory, that defines the \"amount of information\" held in a random variable.**\n",
    "\n",
    "$I_{E}(f)=-\\sum _{i=1}^{J}f_{i}\\log _{2}^{}f_{i}$\n",
    "\n",
    "\n",
    "Information theory, the entropy state function is simply the amount of information that would be needed to specify the full microstate of the system.\n",
    "\n",
    "A key measure in information theory is \"entropy\". **Entropy quantifies the amount of uncertainty(information) involved in the value of a random variable or the outcome of a random process**. \n",
    "\n",
    "Important quantities of information are entropy, **a measure of information in a single random variable**, and mutual information, a measure of information in common between two random variables. \n",
    "\n",
    "The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. A common unit of information is the bit, based on the binary logarithm. \n",
    "\n",
    "Based on the probability mass function of each source symbol to be communicated, the Shannon entropy H, in units of bits (per symbol), is given by\n",
    "\n",
    "$ H=-\\sum _{i}p_{i}\\log _{2}(p_{i})$\n",
    "\n",
    "where $pi$ is the probability of occurrence of the $i-th$ possible value of the source symbol. This equation gives the entropy in the units of \"bits\" (per symbol) because it uses a logarithm of base 2, and this $base-2$ measure of entropy has sometimes been called the \"shannon\" or 'bit'. \n",
    "\n",
    "\n",
    "Intuitively, the entropy HX of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.\n",
    "\n",
    "---\n",
    "\n",
    "$H(X)= -\\sum _{i=1}^{n}p(x_{i})\\log p(x_{i})$\n",
    "\n",
    "$p(x_{i})$ : The probability distribution of the events\n",
    "\n",
    "$\\log p(x_{i})$ : The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources\n",
    "\n",
    "---\n",
    "\n",
    "For instance, the entropy of a coin toss is 1 shannon, whereas of m tosses it is m shannons. Generally, you need log2(n) bits to represent a variable that can take one of n values if n is a power of 2. If these values are equally probable, the entropy (in shannons) is equal to the number of bits. Equality between number of bits and shannons holds only while all outcomes are equally probable. If one of the events is more probable than others, observation of that event is less informative. Conversely, rarer events provide more information when observed.\n",
    "\n",
    "Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is less than log2(n). \n",
    "\n",
    "Entropy is zero when one outcome is certain. \n",
    "\n",
    "Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known. \n",
    "\n",
    "\n",
    "The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.\n",
    "\n",
    "---\n",
    "If, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (more often called bits) have been transmitted. Between these two extremes, information can be quantified as follows. If ùïè is the set of all messages ${x1, ‚Ä¶, xn}$ that $X$ could be, and $p(x)$ is the probability of some $x\\in \\mathbb {X}$ , then the entropy, $H$, of $X$ is defined:\n",
    "\n",
    "$H(X)=\\mathbb {E} _{X}[I(x)]=-\\sum _{x\\in \\mathbb {X} }p(x)\\log p(x)$\n",
    "\n",
    "(Here, $I(x)$ is the self-information, which is the entropy contribution of an individual message, and $ùîºX$ is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable $p(x) = 1/n$; i.e., most unpredictable, in which case $H(X) = log n$\n",
    "\n",
    "\n",
    "The entropy is the expected value of the self-information of the values of a discrete random variable. Sometimes, the entropy itself is called the \"self-information\" of the random variable, possibly because the entropy satisfies $H(X) = \\operatorname I(X; X)$, where $ \\operatorname I(X;X)$ is the mutual information of $X$ with itself.\n",
    "\n",
    "The self-information of a partitioning of elements within a set (or clustering) is the expectation of the information of a test object; if we select an element at random and observe in which partition/cluster it exists, **what quantity of information do we expect to obtain?** The information of a partitioning $C$ with $P(k)$ denoting the fraction of elements within partition $k $ is \n",
    "\n",
    "$I(C) = \\operatorname E( -\\log (\\operatorname P(C))) = -\\sum_{k=1}^n \\operatorname P(k) \\log(\\operatorname P(k))$\n",
    "\n",
    "\n",
    "\n",
    "The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:\n",
    "\n",
    "$H_{\\mathrm {b} }(p)=-p\\log _{2}p-(1-p)\\log _{2}(1-p)$ \n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Binary_entropy_function\n",
    "\n",
    "https://en.wikipedia.org/wiki/Information_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(arr):\n",
    "    categories = Counter(arr)\n",
    "    entropy = []\n",
    "    for k, v in categories.iteritems():\n",
    "        pd = v / float(len(arr)) # Probability distribution\n",
    "        measurement = np.log2(pd)\n",
    "        entropy.append(pd*measurement)\n",
    "    return -1*np.sum(entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 1, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97095059445466858"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**Gain**\n",
    "\n",
    "Intuitively, mutual information measures the information that $X$ and $Y$ share: it measures how much knowing one of these variables reduces uncertainty about the other. For example, if $X$ and $Y$ are independent, then knowing $X$ does not give any information about $Y$ and vice versa, so their mutual information is zero. At the other extreme, if $X$ is a deterministic function of $Y$ and $Y$ is a deterministic function of $X$ then all information conveyed by $X$ is shared with $Y:$ knowing $X$ determines the value of $Y$ and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in $Y$ (or $X$) alone, namely the entropy of $Y$ (or $X$). Moreover, this mutual information is the same as the entropy of $X$ and as the entropy of $Y$. (A very special case of this is when $X$ and $Y$ are the same random variable.)\n",
    "\n",
    "Mutual information is a measure of the inherent dependence expressed in the joint distribution of $X$ and $Y$ relative to the joint distribution of $X$ and $Y$ under the assumption of independence. Mutual information therefore measures dependence in the following sense: $I(X; Y) = 0$ if and only if $X$ and $Y$ are independent random variables. This is easy to see in one direction: if $X$ and $Y$ are independent, then $p(x,y) = p(x) p(y)$, and therefore:\n",
    "\n",
    "$\\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)}=\\log 1=0.\\,\\! $\n",
    "\n",
    "Kullback‚ÄìLeibler divergence (information gain)\n",
    "\n",
    "The Kullback‚ÄìLeibler divergence (or information divergence, information gain, or relative entropy) is a way of comparing two distributions: a \"true\" probability distribution $p(X)$, and an arbitrary probability distribution $q(X)$. If we compress data in a manner that assumes $q(X)$ is the distribution underlying some data, when, in reality, $p(X)$ is the correct distribution, the Kullback‚ÄìLeibler divergence is the number of average additional bits per datum necessary for compression. It is thus defined\n",
    "\n",
    "In information theory and machine learning, information gain is a synonym for Kullback‚ÄìLeibler divergence. However, in the context of decision trees, the term is sometimes used synonymously with mutual information, which is the expected value of the Kullback‚ÄìLeibler divergence of the univariate probability distribution of one variable from the conditional distribution of this variable given the other one.\n",
    "\n",
    "In machine learning, this concept can be used to define a preferred sequence of attributes to investigate to most rapidly narrow down the state of $X$. Such a sequence (which depends on the outcome of the investigation of previous attributes at each stage) is called a decision tree. Usually an attribute with high mutual information should be preferred to other attributes.\n",
    "\n",
    "Information Gain = Entropy(parent) - Weighted Sum of Entropy(Children)\n",
    "\n",
    "$IG(T,a) = H(T) - H(T|a)$\n",
    "\n",
    "Let $T$ denote a set of training examples, each of the form $( x , y ) = ( x 1 , x 2 , x 3 , . . . , x k , y ) $ where $ x_a\\in vals(a)$ is the value of the $ath$ attribute of example $x$ and $y$ is the corresponding class label. The information gain for an attribute $a$ is defined in terms of entropy $H()$ as follows:\n",
    "\n",
    "$IG(T,a) = H(T)-\\sum_{v\\in vals(a)}\\frac{|\\{\\textbf{x}\\in T|x_a=v\\}|}{|T|} \\cdot H(\\{\\textbf{x}\\in T|x_a=v\\})$\n",
    "\n",
    "The mutual information is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. In this case, the relative entropies subtracted from the total entropy are 0.\n",
    "\n",
    "The most common unit of measurement of mutual information is the bit.\n",
    "\n",
    "$Gain (X, SS)= H(X)- \\sum _{ss\\in X}{\\frac {|SS|}{|X|}}H(X)$\n",
    "\n",
    "$SS\\subseteq X$\n",
    "\n",
    "\n",
    "where entropy is\n",
    "\n",
    "$H(X)= -\\sum _{i=1}^{n}p(x_{i})\\log p(x_{i})$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mutual_information\n",
    "\n",
    "https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Gini**\n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n",
    "Gini impurity used by CART classification and regression tree\n",
    "\n",
    "    Information gain = entropy(parent) - average(entropy(child))\n",
    "\n",
    "$ Gini= I_{G}(f)=\\sum _{i=1}^{J}f_{i}(1-f_{i})=\\sum _{i=1}^{J}(f_{i}-{f_{i}}^{2})=\\sum _{i=1}^{J}f_{i}-\\sum _{i=1}^{J}{f_{i}}^{2}=1-\\sum _{i=1}^{J}{f_{i}}^{2}=\\sum _{i\\neq k}f_{i}f_{k}$\n",
    "\n",
    "\n",
    "$ Gini=1-\\sum _{i=1}^{m}{P_{i}}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/eloisaelias/Desktop/weeks/_ipython'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eloisaelias/Desktop\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eloisaelias/Desktop/weeks_SHARED/W4_shared/non-parametric-learners\n"
     ]
    }
   ],
   "source": [
    "cd weeks_SHARED/W4_shared/non-parametric-learners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mArchive\u001b[m\u001b[m/                         k-nearest-neighbors.ipynb\r\n",
      "DecisioTree_elo.py               kNN Demo.ipynb\r\n",
      "DecisionTree_run.py              kNearestNeighbors.py\r\n",
      "TreeNode_elo.py                  kNearestNeighbors.pyc\r\n",
      "\u001b[34mcode\u001b[m\u001b[m/                            lecture.md\r\n",
      "\u001b[34mdata\u001b[m\u001b[m/                            lecture.pdf\r\n",
      "elo_TreeNode.py                  miniquiz.md\r\n",
      "elo_decision_tree.py             miniquiz_soln.md\r\n",
      "elo_knn.py                       \u001b[34mnon-parametric-learners\u001b[m\u001b[m/\r\n",
      "elo_knn.pyc                      \u001b[34mnon-parametric-learnersII\u001b[m\u001b[m/\r\n",
      "elo_knn_.py                      \u001b[34mnon-parametric-learners_l\u001b[m\u001b[m/\r\n",
      "\u001b[34mimages\u001b[m\u001b[m/                          \u001b[34mnon-parametric-learners_t\u001b[m\u001b[m/\r\n",
      "individual.md                    pair.ipynb\r\n",
      "k-nearest-neighbors-Copy1.ipynb  pair.md\r\n",
      "k-nearest-neighbors-Copy2.ipynb  readme.md\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rain</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rain</td>\n",
       "      <td>68</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook  Temperature  Humidity  Windy      Result\n",
       "0     sunny           85        85  False  Don't Play\n",
       "1     sunny           80        90   True  Don't Play\n",
       "2  overcast           83        78  False        Play\n",
       "3      rain           70        96  False        Play\n",
       "4      rain           68        80  False        Play"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 5 columns):\n",
      "Outlook        14 non-null object\n",
      "Temperature    14 non-null int64\n",
      "Humidity       14 non-null int64\n",
      "Windy          14 non-null bool\n",
      "Result         14 non-null object\n",
      "dtypes: bool(1), int64(2), object(2)\n",
      "memory usage: 534.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Outlook = df.Outlook.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>Don't Play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>Play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outlook  Temperature  Humidity  Windy      Result\n",
       "0        2           85        85  False  Don't Play\n",
       "1        2           80        90   True  Don't Play\n",
       "2        0           83        78  False        Play"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Don't Play\", 'Play'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Result.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Result = df.Result.map({\"Don't Play\": False, 'Play':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2      True\n",
       "3      True\n",
       "4      True\n",
       "5     False\n",
       "6      True\n",
       "7     False\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11     True\n",
       "12     True\n",
       "13    False\n",
       "Name: Result, dtype: bool"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outlook  Temperature  Humidity  Windy Result\n",
       "0        2           85        85  False  False\n",
       "1        2           80        90   True  False\n",
       "2        0           83        78  False   True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.pop('Result').as_matrix()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 85, 85, False],\n",
       "       [2, 80, 90, True],\n",
       "       [0, 83, 78, False],\n",
       "       [1, 70, 96, False],\n",
       "       [1, 68, 80, False],\n",
       "       [1, 65, 70, True],\n",
       "       [0, 64, 65, True],\n",
       "       [2, 72, 95, False],\n",
       "       [2, 69, 70, False],\n",
       "       [1, 75, 80, False],\n",
       "       [2, 75, 70, True],\n",
       "       [0, 72, 90, True],\n",
       "       [0, 81, 75, False],\n",
       "       [1, 71, 80, True]], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.as_matrix()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy') # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train) # fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(clf, out_file='elo.dot') # export decision tree in dot format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## generate png image  \n",
    "%%bash\n",
    "dot -Tpng elo.dot -o elo.png "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_gini():\n",
    "    array = [1, 1, 2, 1, 2]\n",
    "    result = _gini(np.array(array))\n",
    "    actual = 0.48\n",
    "    message = 'Gini value for {}: Got {}. Should be {}'.format(array, result, actual)\n",
    "    n.assert_almost_equal(result, actual, 4, message)\n",
    "    print message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gini(y):\n",
    "    categories = Counter(y)\n",
    "    print categories\n",
    "    gini = []\n",
    "    for k, v in categories.iteritems():\n",
    "        p_y = v/float(len(y))\n",
    "        print p_y\n",
    "        gini.append(p_y**2)\n",
    "\n",
    "    return 1 - sum(gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = np.array([1, 1, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 3, 2: 2})\n",
      "0.6\n",
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gini(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**--Extended**\n",
    "\n",
    "**Entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of unpredictability of information content. To get an intuitive understanding of these three terms, consider the example of a political poll. Usually, such polls happen because the outcome of the poll isn't already known. In other words, the outcome of the poll is relatively unpredictable, and actually performing the poll and learning the results gives some new information; these are just different ways of saying that the entropy of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the entropy of the second poll result is small relative to the first.\n",
    "\n",
    "Now consider the example of a coin toss. Assuming the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time: the best we can do is predict that the coin will come up heads, and our prediction will be correct with probability $1/2$. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. Contrarily, a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, one binary bit has a $\\log _{2}2=1$ Shannon or bit entropy because it can have one of two values (1 and 0). Similarly, one trit contains $\\log _{2}3$ (about 1.58496) bits of information because it can have one of three values.\n",
    "\n",
    "\n",
    "English text, treated as a string of characters, has fairly low entropy, i.e., is fairly predictable. Even if we do not know exactly what is going to come next, we can be fairly certain that, for example, there will be many more e's than z's, that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy for each character of message\n",
    "\n",
    "If a compression scheme is lossless‚Äîthat is, you can always recover the entire original message by decompressing‚Äîthen a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. \n",
    "\n",
    "This means a compressed message has less redundancy. Roughly speaking, Shannon's source coding theorem says that a lossless compression scheme cannot compress messages, on average, to have more than one bit of information per bit of message, but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.\n",
    "\n",
    "***\n",
    "Intuitively, imagine that we wish to transmit sequences one of the 4 characters A, B, C, or D. Thus, a message to be transmitted might be 'ABADDCAB'. Information theory gives a way of calculating the smallest possible amount of information that will convey this. If all 4 letters are equally likely (25%), we can do no better (over a binary channel) than to have 2 bits encode (in binary) each letter: A might code as '00', B as '01', C as '10', and D as '11'. Now suppose A occurs with 70% probability, B with 26%, and C and D with 2% each. We could assign variable length codes, so that receiving a '1' tells us to look at another bit unless we have already received 2 bits of sequential 1's. In this case, A would be coded as '0' (one bit), B as '10', and C and D as '110' and '111'. It is easy to see that 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, then, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of A followed by B - together 96% of characters). The calculation of the sum of probability-weighted log probabilities measures and captures this effect.\n",
    "\n",
    "Shannon's theorem also implies that no lossless compression scheme can shorten all messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, MP4, PNG or JPEG will generally result in a ZIP file that is slightly larger than the source file(s).\n",
    "\n",
    "Named after Boltzmann's Œó-theorem, Shannon defined the entropy Œó (Greek letter Eta) of a discrete random variable X with possible values {x1, ‚Ä¶, xn} and probability mass function P(X) as:\n",
    "\n",
    "$\\mathrm {H} (X)=\\mathrm {E} [\\mathrm {I} (X)]=\\mathrm {E} [-\\ln(\\mathrm {P} (X))]$\n",
    "\n",
    "Here E is the expected value operator, and I is the information content of X.[4][5] I(X) is itself a random variable.\n",
    "\n",
    "\n",
    "The entropy can explicitly be written as\n",
    "\n",
    "$\\mathrm {H} (X)=\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\,\\mathrm {I} (x_{i})}=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{b}\\mathrm {P} (x_{i})}$\n",
    "\n",
    "where b is the base of the logarithm used. Common values of b are 2, Euler's number e, and 10, and the unit of entropy is shannon for b = 2, nat for b = e, and hartley for b = 10.[6] When b = 2, the units of entropy are also commonly referred to as bits.\n",
    "\n",
    "In the case of P(xi) = 0 for some i, the value of the corresponding summand 0 logb(0) is taken to be 0, which is consistent with the limit:\n",
    "\n",
    "$\\lim _{p\\to 0+}p\\log(p)=0$\n",
    "\n",
    "When the distribution is continuous rather than discrete, the sum is replaced with an integral as\n",
    "\n",
    "$\\mathrm {H} (X)=\\int {\\mathrm {P} (x)\\,\\mathrm {I} (x)}~dx=-\\int {\\mathrm {P} (x)\\log _{b}\\mathrm {P} (x)}~dx$\n",
    "\n",
    "where $P(x)$ represents a probability density function.\n",
    "\n",
    "One may also define the conditional entropy of two events $X$ and $Y$ taking values $xi$ and $yj$ respectively, as\n",
    "\n",
    "$ \\mathrm {H} (X|Y)=\\sum _{i,j}p(x_{i},y_{j})\\log {\\frac {p(y_{j})}{p(x_{i},y_{j})}}$\n",
    "\n",
    "where $p(xi, yj)$ is the probability that $X = xi$ and $Y = yj$. This quantity should be understood as the amount of randomness in the random variable $X$ given the event $Y$.\n",
    "\n",
    "\n",
    "To understand the meaning of $‚àë pi log(1/pi)$, at first, try to define an information function, I, in terms of an event i with probability pi. How much information is acquired due to the observation of event i? Shannon's solution follows from the fundamental properties of information:[7]\n",
    "\n",
    "$I(p)$ is monotonic ‚Äì increases and decreases in the probability of an event produces increases and decreases in information, respectively\n",
    "\n",
    "$I(p) ‚â• 0$ ‚Äì information is a non-negative quantity\n",
    "\n",
    "$I(1) = 0$ ‚Äì events that always occur do not communicate information\n",
    "\n",
    "$I(p1 p2) = I(p1) + I(p2)$ ‚Äì information due to independent events is additive\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
